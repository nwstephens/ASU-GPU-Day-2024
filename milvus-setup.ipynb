{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use bulkInsert to test GPU index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Docker installation: https://docs.docker.com/engine/install/ubuntu/\n",
    "2. Install nvidia-docker2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\n",
    "distribution=$(. /etc/os-release;echo $ID$VERSION_ID)\n",
    "curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n",
    "sudo apt-get update\n",
    "sudo apt-get install nvidia-docker2\n",
    "sudo systemctl restart docker.service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install NVIDIA driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install --no-install-recommends  nvidia-headless-535 nvidia-utils-535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (Optional) mount a high performance disk for test. We need to ensure all following operations are in a high performance disk. For an AWS host, we need to manually mount the NVMe SSD (For example, g4dn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsblk # see device path\n",
    "sudo mkfs -t ext4 /dev/nvme1n1\n",
    "sudo mkdir /data\n",
    "sudo mount /dev/nvme1n1 /data\n",
    "sudo -i blkid # get /dev/nvme1n1 UUID, e.g. dd04113f-deb6-42b0-a021-03110c119295 \n",
    "sudo vi /etc/fstab # add to the tail: UUID=<UUID get from previous cmd> /data ext4 defaults 1 2\n",
    "cd /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download milvus image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker pull milvusdb/milvus:v2.4.0-rc.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use docker compose to start the milvus service "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the following file as docker-compose.yml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version: '3.5'\n",
    "\n",
    "services:\n",
    "  etcd:\n",
    "    container_name: milvus-etcd\n",
    "    image: quay.io/coreos/etcd:v3.5.5\n",
    "    environment:\n",
    "      - ETCD_AUTO_COMPACTION_MODE=revision\n",
    "      - ETCD_AUTO_COMPACTION_RETENTION=1000\n",
    "      - ETCD_QUOTA_BACKEND_BYTES=4294967296\n",
    "      - ETCD_SNAPSHOT_COUNT=50000\n",
    "    volumes:\n",
    "      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd\n",
    "    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"etcdctl\", \"endpoint\", \"health\"]\n",
    "\n",
    "  minio:\n",
    "    container_name: milvus-minio\n",
    "    image: minio/minio:RELEASE.2023-03-20T20-16-18Z\n",
    "    environment:\n",
    "      MINIO_ACCESS_KEY: minioadmin\n",
    "      MINIO_SECRET_KEY: minioadmin\n",
    "    ports:\n",
    "      - \"9001:9001\"\n",
    "      - \"9000:9000\"\n",
    "    volumes:\n",
    "      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data\n",
    "    command: minio server /minio_data --console-address \":9001\"\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n",
    "      interval: 30s\n",
    "      timeout: 20s\n",
    "      retries: 3\n",
    "\n",
    "  standalone:\n",
    "    container_name: milvus-standalone\n",
    "    image: milvusdb/milvus:v2.4.0.1-gpu-beta\n",
    "    command: [\"milvus\", \"run\", \"standalone\"]\n",
    "    environment:\n",
    "      ETCD_ENDPOINTS: etcd:2379\n",
    "      MINIO_ADDRESS: minio:9000\n",
    "    volumes:\n",
    "      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus\n",
    "    ports:\n",
    "      - \"19530:19530\"\n",
    "      - \"9091:9091\"\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              capabilities: [\"gpu\"]\n",
    "              device_ids: [\"0\"]\n",
    "    depends_on:\n",
    "      - \"etcd\"\n",
    "      - \"minio\"\n",
    "\n",
    "networks:\n",
    "  default:\n",
    "    name: milvus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cmd: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We may need to use sudo since we are in /data, and all pip command need to be under sudo.\n",
    "Set the dataset as an environment variableï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export DATASET=\"cohere\" # or \"openai\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip3 install polars\n",
    "pip3 install numpy\n",
    "pip3 install s3fs\n",
    "pip3 install environs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import s3fs\n",
    "import environs\n",
    "env = environs.Env()\n",
    "env.read_env(\".env\")\n",
    "dataset_name = env.str(\"DATASET\", \"cohere\")\n",
    "parquet_path = dataset_name + \"_data/\"\n",
    "npy_path = dataset_name + \"_npy_data/\"\n",
    "\n",
    "base_file = parquet_path + \"shuffle_train.parquet\"\n",
    "query_file = parquet_path + \"test.parquet\"\n",
    "output_base = npy_path + \"base.npy\"\n",
    "output_id = npy_path + \"id.npy\"\n",
    "try: \n",
    "    shutil.rmtree(parquet_path)\n",
    "except:\n",
    "    pass   \n",
    "try:\n",
    "    shutil.rmtree(npy_path)\n",
    "except:\n",
    "    pass\n",
    "os.mkdir(parquet_path)\n",
    "os.mkdir(npy_path)\n",
    "#download s3 file\n",
    "fs = s3fs.S3FileSystem(anon=True, client_kwargs={\"region_name\": \"us-west-2\"})\n",
    "if dataset_name == \"cohere\":\n",
    "    s3_path = \"assets.zilliz.com/benchmark/cohere_medium_1m\"\n",
    "elif dataset_name == \"openai\":\n",
    "    s3_path = \"assets.zilliz.com/benchmark/openai_medium_500k\"\n",
    "dataset_info = fs.ls(s3_path, detail=True)\n",
    "\n",
    "downloads = []\n",
    "for info in dataset_info:\n",
    "    downloads.append(info['Key'])\n",
    "print(\"download files:\", downloads)\n",
    "fs.download(downloads, parquet_path)\n",
    "\n",
    "df_train = pl.read_parquet(base_file)\n",
    "base = np.stack(df_train['emb']).astype(np.float32)\n",
    "id = np.stack(df_train['id']).astype(np.int64)\n",
    "all_embeddings = base / np.linalg.norm(base, axis=1)[:, np.newaxis]\n",
    "np.save(output_base, all_embeddings)\n",
    "np.save(output_id, id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for bulkinsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip3 install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import CollectionSchema, FieldSchema, DataType, utility, connections, Collection, list_collections\n",
    "\n",
    "import time\n",
    "import struct\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "\n",
    "import environs\n",
    "env = environs.Env()\n",
    "env.read_env(\".env\")\n",
    "dataset_name = env.str(\"DATASET\", \"cohere\")\n",
    "data_minio_path = dataset_name + \"_npy_data/\"\n",
    "\n",
    "collection_name = \"VectorDBBenchCollection\"\n",
    "id_path = data_minio_path + \"/id.npy\"\n",
    "data_path = data_minio_path + \"/base.npy\"\n",
    "def get_base_shape():\n",
    "    base_data = np.load(data_path, mmap_mode='r')\n",
    "    return base_data.shape\n",
    "(nb, dim) = get_base_shape()\n",
    "connections.connect(host=\"localhost\", port=19530)\n",
    "\n",
    "# minio\n",
    "DEFAULT_BUCKET_NAME = \"a-bucket\"\n",
    "MINIO_ADDRESS = \"0.0.0.0:9000\"\n",
    "MINIO_SECRET_KEY = \"minioadmin\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "\n",
    "def upload(data_folder: str,\n",
    "           bucket_name: str=DEFAULT_BUCKET_NAME)->(bool, list):\n",
    "    if not os.path.exists(data_folder):\n",
    "        print(\"Data path '{}' doesn't exist\".format(data_folder))\n",
    "        return False, []\n",
    "\n",
    "    remote_files = []\n",
    "    try:\n",
    "        print(\"Prepare upload files\")\n",
    "        minio_client = Minio(endpoint=MINIO_ADDRESS, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY, secure=False)\n",
    "        found = minio_client.bucket_exists(bucket_name)\n",
    "        if not found:\n",
    "            print(\"MinIO bucket '{}' doesn't exist\".format(bucket_name))\n",
    "            return False, []\n",
    "\n",
    "        remote_data_path = \"milvus_bulkinsert\"\n",
    "        def upload_files(folder:str):\n",
    "            for parent, dirnames, filenames in os.walk(folder):\n",
    "                if parent is folder:\n",
    "                    for filename in filenames:\n",
    "                        ext = os.path.splitext(filename)\n",
    "                        if len(ext) != 2 or (ext[1] != \".json\" and ext[1] != \".npy\"):\n",
    "                            continue\n",
    "                        local_full_path = os.path.join(parent, filename)\n",
    "                        minio_file_path = os.path.join(remote_data_path, os.path.basename(folder), filename)\n",
    "                        minio_client.fput_object(bucket_name, minio_file_path, local_full_path)\n",
    "                        print(\"Upload file '{}' to '{}'\".format(local_full_path, minio_file_path))\n",
    "                        remote_files.append(minio_file_path)\n",
    "                    for dir in dirnames:\n",
    "                        upload_files(os.path.join(parent, dir))\n",
    "\n",
    "        upload_files(data_folder)\n",
    "\n",
    "    except S3Error as e:\n",
    "        print(\"Failed to connect MinIO server {}, error: {}\".format(MINIO_ADDRESS, e))\n",
    "        return False, []\n",
    "\n",
    "    print(\"Successfully upload files: {}\".format(remote_files))\n",
    "    return True, remote_files\n",
    "\n",
    "print(f\"\\nList collections...\")\n",
    "collection_list = list_collections()\n",
    "print(list_collections())\n",
    "\n",
    "if(collection_list.count(collection_name)):\n",
    "    print(collection_name, \" exist, and drop it\")\n",
    "    collection = Collection(collection_name)\n",
    "    collection.drop()\n",
    "    print(\"drop\")\n",
    "print(\"create collection\")\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"base\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields)\n",
    "\n",
    "coll = Collection(collection_name, schema)\n",
    "\n",
    "begin_t = time.time()\n",
    "ok, remote_files = upload(data_folder=data_minio_path)\n",
    "\n",
    "print(\"do_bulk_insert\")\n",
    "task_id = utility.do_bulk_insert(\n",
    "    collection_name=collection_name,\n",
    "    files=remote_files)\n",
    "\n",
    "print(\"wait insert\")\n",
    "while True:\n",
    "    if coll.num_entities == nb:\n",
    "        coll.flush()\n",
    "        break\n",
    "    time.sleep(1)\n",
    "insert_t  = time.time()\n",
    "print(\"bulk insert time:\", insert_t - begin_t)\n",
    "\n",
    "print(\"create index\")\n",
    "try:\n",
    "    coll.create_index(field_name=\"base\",\n",
    "        index_params={'index_type': 'GPU_CAGRA',  \n",
    "            'metric_type': 'L2',\n",
    "            'params': {\n",
    "                'intermediate_graph_degree':64,\n",
    "                'graph_degree': 32,\n",
    "                'M':14,\n",
    "                'efConstruction': 360,\n",
    "                \"nlist\":1024,\n",
    "                }})\n",
    "except Exception as e:\n",
    "    print(f\"index error: {e}\")\n",
    "    raise e from None\n",
    "\n",
    "def wait_index():\n",
    "    while True:\n",
    "        progress = utility.index_building_progress(collection_name)\n",
    "        print(progress)\n",
    "        if progress.get(\"pending_index_rows\", -1) == 0:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        \n",
    "print(\"wait index\")\n",
    "wait_index()\n",
    "index_t  = time.time()\n",
    "print(\"create index time :\", index_t - insert_t)\n",
    "\n",
    "print(\"load index\")\n",
    "coll.load()\n",
    "load_t = time.time()\n",
    "print(\"load index time :\", load_t - index_t)\n",
    "print(\"total time:\", load_t - begin_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for recall and QPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Pool\n",
    "from pymilvus import CollectionSchema, FieldSchema, DataType, utility, connections, Collection\n",
    "import time\n",
    "import struct\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import environs\n",
    "env = environs.Env()\n",
    "env.read_env(\".env\")\n",
    "dataset_name = env.str(\"DATASET\", \"cohere\")\n",
    "data_path = dataset_name + \"_data/\"\n",
    "\n",
    "gt_path = data_path + \"/neighbors.parquet\"\n",
    "queries_path = data_path + \"/test.parquet\"\n",
    "topk = 10\n",
    "search_params = {\n",
    "    \"metric_type\": \"L2\", \n",
    "    \"params\": {\n",
    "        \"ef\": 200, # 200 for Cohere, 100 for OpenAI to get 98% recall\n",
    "        \"search_width\":1,\n",
    "        \"itopk_size\": 128,\n",
    "        \"min_iterations\":0,\n",
    "        \"max_iterations\":34, # 34 for Cohere, 19 for OpenAI to get 98% recall\n",
    "        }\n",
    "    }\n",
    "collection_name = \"VectorDBBenchCollection\"\n",
    "connections.connect(host=\"localhost\", port=19530)\n",
    "\n",
    "def get_gt():\n",
    "    ans = pl.read_parquet(gt_path)[\"neighbors_id\"]\n",
    "    res = []\n",
    "    print(len(ans), type(ans))\n",
    "    for i in range(len(ans)):\n",
    "        one_ans = [id for id in ans[i]]\n",
    "        res.append(one_ans)\n",
    "    return res \n",
    "\n",
    "def get_total_quries():\n",
    "    test_data =  pl.read_parquet(queries_path)\n",
    "    test_emb = np.stack(test_data[\"emb\"]).astype(np.float32)\n",
    "    test_emb = test_emb / np.linalg.norm(test_emb, axis=1)[:, np.newaxis]\n",
    "    test_emb = test_emb.tolist()\n",
    "    return test_emb\n",
    "\n",
    "def get_queries(nq):\n",
    "    test_emb = get_total_quries()\n",
    "    num = len(test_emb)\n",
    "    idx = 0\n",
    "    queries = []\n",
    "    while idx < num:\n",
    "        queries.append(test_emb[idx:idx + nq])\n",
    "        idx = idx + nq\n",
    "    return queries\n",
    "\n",
    "def search(search_params, loop, limit = -1):\n",
    "    queries = get_total_quries()\n",
    "    collection = Collection(collection_name)\n",
    "    collection.load()\n",
    "    print(\"loaded\")\n",
    "    to_query = queries\n",
    "    if (limit > 0):\n",
    "        to_query = queries[0: limit]\n",
    "    start = time.time()\n",
    "    passed = 0.0\n",
    "    index = 0\n",
    "    gt = get_gt()\n",
    "    recall = 0.0\n",
    "    for i in range(loop):\n",
    "        for query in to_query:\n",
    "            t1 = time.time()\n",
    "            results = collection.search(\n",
    "                data=[query], \n",
    "                anns_field=\"base\", \n",
    "                param=search_params, \n",
    "                limit=topk, \n",
    "                expr=None,\n",
    "                consistency_level=\"Eventually\"\n",
    "            )\n",
    "            passed += (time.time() - t1)\n",
    "            hit = 0\n",
    "            for r in results:\n",
    "                for t in r:\n",
    "                    if t.id in gt[index][0:topk]:\n",
    "                        hit += 1\n",
    "            recall += float(hit) / topk\n",
    "            index +=1\n",
    "        print (\"recall:\", recall / float(len(to_query)))\n",
    "        print(\"time usage: \" + str(time.time() - start) + \", latency: \" + str(passed / len(to_query) / (i + 1)) + \", qps: \" + str(len(to_query) * (i + 1) / passed))  \n",
    "\n",
    "def non_stop_search(name, search_params, queries, run_time):\n",
    "    collection = Collection(name)\n",
    "    nq = 0\n",
    "    passed = 0\n",
    "    while (True):\n",
    "        for query in queries:\n",
    "            t1 = time.time()\n",
    "            results = collection.search(\n",
    "                data=query, \n",
    "                anns_field=\"base\", \n",
    "                param=search_params, \n",
    "                limit=topk, \n",
    "                expr=None,\n",
    "                consistency_level=\"Eventually\"\n",
    "            )\n",
    "            nq += 1\n",
    "            passed += (time.time() - t1)\n",
    "            if (passed > run_time):\n",
    "                return [nq, passed]\n",
    "\n",
    "def parallel_search(name, search_params, queries, num_threads, run_time):\n",
    "    pool = Pool(num_threads)\n",
    "\n",
    "    inputs = []\n",
    "    for i in range(num_threads):\n",
    "        inputs.append((name, search_params, queries, run_time))\n",
    "    t1 = time.time()\n",
    "    outputs = pool.starmap(non_stop_search, inputs)\n",
    "    t2 = time.time()\n",
    "\n",
    "    sumq = 0\n",
    "    sumt = 0.0\n",
    "    for re in outputs:\n",
    "        sumq += re[0]\n",
    "        sumt += re[1]\n",
    "    print(\"time usage: \" + str(t2 - t1) + \", latency: \" + str(sumt / sumq) + \", qps: \" + str(sumq * num_threads / sumt))  \n",
    "\n",
    "def do():\n",
    "    collection = Collection(collection_name)\n",
    "    collection.load()\n",
    "    print(\"loaded\")\n",
    "    for nq in [1, 10, 100]:\n",
    "        print(\"Run for NQ: \", nq)\n",
    "        queries = get_queries(nq)\n",
    "        parallel_search(collection_name, search_params, queries, num_threads = 500, run_time = 60)\n",
    "\n",
    "print(\"serial execution to get search recall:\")\n",
    "search(search_params, 1)\n",
    "print(\"concurrent execution(50 threads) to get search qps:\")\n",
    "do()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
